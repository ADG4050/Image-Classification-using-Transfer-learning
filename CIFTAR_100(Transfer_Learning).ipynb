{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a45Qt8wh2q8z"
      },
      "outputs": [],
      "source": [
        "# Import the necessary modules\n",
        "\n",
        "import keras\n",
        "from tensorflow.keras import datasets\n",
        "from tensorflow.keras.layers import Input, Dense, Flatten, Dropout, Activation \n",
        "from tensorflow.keras.layers import PReLU, LeakyReLU, Conv2D, Lambda, MaxPooling2D\n",
        "from tensorflow.keras.regularizers import l2\n",
        "\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "\n",
        "from tensorflow.keras.models import model_from_json\n",
        "\n",
        "from IPython.display import clear_output\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from matplotlib.ticker import MaxNLocator\n",
        "\n",
        "import pickle\n",
        "import sklearn as skl\n",
        "\n",
        "from sklearn import datasets, linear_model\n",
        "from sklearn.model_selection import cross_val_score"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define some useful functions\n",
        "class PlotLossAccuracy(keras.callbacks.Callback):\n",
        "    def on_train_begin(self, logs={}):\n",
        "        self.i = 0\n",
        "        self.x = []\n",
        "        self.acc = []\n",
        "        self.losses = []\n",
        "        self.val_losses = []\n",
        "        self.val_acc = []\n",
        "        self.logs = []\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        \n",
        "        self.logs.append(logs)\n",
        "        self.x.append(int(self.i))\n",
        "        self.losses.append(logs.get('loss'))\n",
        "        self.val_losses.append(logs.get('val_loss'))\n",
        "        self.acc.append(logs.get('accuracy'))\n",
        "        self.val_acc.append(logs.get('val_accuracy'))\n",
        "        \n",
        "        self.i += 1\n",
        "        \n",
        "        clear_output(wait=True)\n",
        "        plt.figure(figsize=(16, 6))\n",
        "        plt.plot([1, 2])\n",
        "        plt.subplot(121) \n",
        "        plt.plot(self.x, self.losses, label=\"train loss\")\n",
        "        plt.plot(self.x, self.val_losses, label=\"validation loss\")\n",
        "        plt.gca().xaxis.set_major_locator(MaxNLocator(integer=True))\n",
        "        plt.ylabel('loss')\n",
        "        plt.xlabel('epoch')\n",
        "        plt.title('Model Loss')\n",
        "        plt.legend()\n",
        "        plt.subplot(122)         \n",
        "        plt.plot(self.x, self.acc, label=\"training accuracy\")\n",
        "        plt.plot(self.x, self.val_acc, label=\"validation accuracy\")\n",
        "        plt.legend()\n",
        "        plt.ylabel('accuracy')\n",
        "        plt.xlabel('epoch')\n",
        "        plt.title('Model Accuracy')\n",
        "        plt.gca().xaxis.set_major_locator(MaxNLocator(integer=True))\n",
        "        plt.show();\n",
        "        \n",
        "def save_top_model_to_disk():    \n",
        "    # save model and weights (don't change the filenames)\n",
        "    model_json = top_model.to_json()\n",
        "    with open(\"top_model.json\", \"w\") as json_file:\n",
        "        json_file.write(model_json)\n",
        "    # serialize weights to HDF5\n",
        "    top_model.save_weights(\"top_model.h5\")\n",
        "    print(\"Saved top_model to top_model.json and weights to top_model.h5\")\n",
        "\n",
        "def save_finetuned_model_to_disk():    \n",
        "    # save model and weights (don't change the filenames)\n",
        "    model_json = finetuned_model.to_json()\n",
        "    with open(\"finetuned_model.json\", \"w\") as json_file:\n",
        "        json_file.write(model_json)\n",
        "    # serialize weights to HDF5\n",
        "    finetuned_model.save_weights(\"finetuned_model.h5\")\n",
        "    print(\"Saved model to finetuned_model.json and weights to finetuned_model.h5\")\n",
        "\n"
      ],
      "metadata": {
        "id": "qElJodb-20bz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!curl --create-dirs -o /home/tcd/data/cifar100-dataset.pkl http://4c16.ml/~mindfreeze/cifar100-dataset.pkl"
      ],
      "metadata": {
        "id": "VWHYrK3q279g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading new dataset cifar100\n",
        "\n",
        "print('loading the dataset...')\n",
        "\n",
        "pkl_file = open('/home/tcd/data/cifar100-dataset.pkl', 'rb')\n",
        "dataset = pickle.load(pkl_file)\n",
        "\n",
        "print('loaded.')\n",
        "\n",
        "print('let\\'s look at some of the pictures and their ground truth labels:')\n",
        "\n",
        "plt.figure(figsize=(12,12))\n",
        "plt.plot([3, 3])\n",
        "\n",
        "X = dataset['X'].astype('float32')/255\n",
        "Y = dataset['Y'].astype('float32')\n",
        "Y = keras.utils.to_categorical(Y)\n",
        "\n",
        "for i in range(0,9):\n",
        "    # pictures are 32x32x3 (width=32, height=32, 3 colour channels)\n",
        "    pic = X[i]\n",
        "\n",
        "    # Y[i] returns an array of zeros and with Y[i][classid] = 1\n",
        "    # for instance  Y[i] = [ 0 0 0 0 0 1 0 0 0 0] => classid=5 \n",
        "    #          and  Y[i] = [ 1 0 0 0 0 0 0 0 0 0] => classid=0\n",
        "    # we can get the classid by using the argmax function on the vector Y[i]\n",
        "    classid = Y[i].argmax(-1)\n",
        "\n",
        "    # getting back the name of the label for that classid\n",
        "    classname = dataset['labels'][classid]\n",
        "    plt.subplot(3,3,i+1)\n",
        "    plt.imshow(pic)\n",
        "    plt.title('label: {}'.format(classname))\n"
      ],
      "metadata": {
        "id": "WT0DRNX73ABn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# training / validation split\n",
        "X_train, X_validation, Y_train, Y_validation = skl.model_selection.train_test_split(X, Y, test_size=.1)"
      ],
      "metadata": {
        "id": "fh4Mx-ZL3FfY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Here we assume that you have already successfully trained a CNN on cifar10\n",
        "\n",
        "# load json and create base_model\n",
        "with open('../model.json', 'r') as json_file:\n",
        "    loaded_model_json = json_file.read()\n",
        "base_model = model_from_json(loaded_model_json)\n",
        "# load weights into base_model\n",
        "base_model.load_weights(\"../model.h5\")\n",
        "# print the model summary"
      ],
      "metadata": {
        "id": "fJasXWxQ3Jj3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_model.summary()\n",
        "\n",
        "# In the summary, take note of the Layer names in the first column\n",
        "# eg. conv2d_6 (Conv2D) means that 'conv2d_6' will be the name for \n",
        "# that layer\n",
        "\n",
        "# In preparation for the next step, You also need to check that \n",
        "# shape of the Flatten layer is small enough. Typically we want something\n",
        "# like (None, 2048). You don't want (None, 40000)."
      ],
      "metadata": {
        "id": "QGMYdyiP3YYW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# change the `flatten_2` name to the corresponding name from your network\n",
        "\n",
        "# TODO: create model to predict the features at 'flatten' layer\n",
        "\n",
        "new_model = keras.Model(inputs = base_model.get_layer('input_2').input, outputs = base_model.get_layer('flatten_1').output)\n",
        "train_features = new_model.predict(X_train)\n",
        "validation_features = new_model.predict(X_validation)\n",
        "\n",
        "# checking that everything is ok, you should get a tensor of size 4500 x Flatten Size\n",
        "# eg. (4500, 2048)\n",
        "print(train_features.shape)"
      ],
      "metadata": {
        "id": "s6lJVcXX3eEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# now that you have computed the new features for your dataset,\n",
        "# you need to design a simple classification network.\n",
        "# For good measure, use one with BatchNormalization, dropout, etc. \n",
        "# and train it.\n",
        "\n",
        "\n",
        "inputs = keras.layers.Input(shape=train_features.shape[1:]) \n",
        "x = inputs\n",
        "\n",
        "x = Dense(600, activation = 'relu')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Dropout(0.1)(x)\n",
        "x = Dense(500, activation = 'relu')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Dropout(0.2)(x)\n",
        "x = Dense(500, activation = 'relu')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Dropout(0.2)(x)\n",
        "\n",
        "# TODO: complete the network ....\n",
        "predictions = Dense(10, activation='softmax')(x)\n",
        "\n"
      ],
      "metadata": {
        "id": "1lFs9krA3iy8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# now we can compile it and train it.\n",
        "\n",
        "top_model = keras.models.Model(inputs=inputs, outputs=predictions)\n",
        "\n",
        "# TODO: write code to train it ...\n",
        "\n",
        "top_model = keras.models.Model(inputs=inputs, outputs=predictions)\n",
        "opt = keras.optimizers.Adam(learning_rate=0.0005)\n",
        "#, decay=1e-6, momentum=0.9, nesterov=True\n",
        "\n",
        "# setup the optimisation strategy\n",
        "top_model.compile(optimizer=opt,\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "\n",
        "top_model.summary()\n",
        "\n",
        "if (top_model.count_params() > 10000000):    \n",
        "    raise(\"Your model is unecessarily complex, scale down!\")\n",
        "\n",
        "# the dataset is very small and the base_model gives us a very good \n",
        "# starting point, so we only need a handful of epochs, probably 1 \n",
        "# or 2 will do.\n",
        "\n",
        "pltCallBack = PlotLossAccuracy()\n",
        "\n",
        "# and train\n",
        "top_model.fit(train_features, Y_train,\n",
        "          batch_size=1024, epochs=15, \n",
        "          validation_data=(validation_features, Y_validation), \n",
        "          callbacks=[pltCallBack])"
      ],
      "metadata": {
        "id": "JBNkZeIG3o7f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "finetuned_model = keras.models.clone_model(base_model)\n",
        "\n",
        "# set the all the layers in model to trainable = False\n",
        "\n",
        "for layer in finetuned_model.layers:\n",
        "    layer.trainable = False\n",
        "    \n",
        "# TODO: then set all the layers after the Flatten layer to be trainable \n",
        "finetuned_model.layers[12].trainable = True\n",
        "finetuned_model.layers[13].trainable = True\n",
        "finetuned_model.layers[14].trainable = True\n",
        "finetuned_model.layers[15].trainable = True\n",
        "finetuned_model.layers[16].trainable = True"
      ],
      "metadata": {
        "id": "Xu2DfZOV3x5l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO compile model \n",
        "\n",
        "# finetuned_model.compile(...)\n",
        "finetuned_model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "finetuned_model.summary()\n",
        "# finetuned_model.summary(...)\n",
        "\n",
        "# loading the pre-trained weights from your lab-05 network\n",
        "finetuned_model.load_weights(\"../model.h5\")"
      ],
      "metadata": {
        "id": "LB8VsuRV37nX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO train 'model' on (X_train, Y_train) for 10 or so epochs  \n",
        "# What you should be able to observe is that the training is a bit slower than\n",
        "# in the previous case. This is partly because we haven't re-initialised the weights.\n",
        "num_epochs = 14\n",
        "\n",
        "pltCallBack = PlotLossAccuracy()\n",
        "\n",
        "finetuned_model.fit(X_train, Y_train,\n",
        "          batch_size=1024, epochs=num_epochs, \n",
        "          validation_data=(X_validation, Y_validation), \n",
        "          callbacks=[pltCallBack])"
      ],
      "metadata": {
        "id": "ZJ1UJ6Er4CeF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}